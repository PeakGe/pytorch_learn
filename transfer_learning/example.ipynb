{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 迁移学习\n",
    "\n",
    "在深度学习实践中，我们很少有人会从头开始训练整个卷积网络（随机初始化权重），因为要从头训练一个卷积网络是需要足够多的数据的，而通常数据非常难获取。**相反，我们直接下载别人已经训练好的某个网络结构的权重，用这个作为预训练，然后通过简单的微调模型切换到你感兴趣的任务上，将会进展的非常快。**大多数的Kaggle竞赛的解决方案都是如此。\n",
    "\n",
    "比如ImageNet，或者MS COCO，或者Pascal类型的数据集，这些都是不同数据集的名字，它们都是由大家上传到网络的，并且有大量的计算机视觉研究者已经用这些数据集训练过他们的算法了。有时候这些训练过程需要花费好几周，并且需要很多的GPU，其它人已经做过了，并且经历了非常痛苦的寻最优过程，**这就意味着你可以下载花费了别人好几周甚至几个月而做出来的开源的权重参数，把它当作一个很好的初始化用在你自己的神经网络上。**\n",
    "\n",
    "假设有一个在ImageNet数据集（140万张标记图像，1000个不同的类别）上训练好的大型卷积神经网络 ImageNet中包含许多动物类别，其中包括不同种类的蚂蚁和蜜蜂，因此可以认为它在蚂蚁和蜜蜂分类问题上也能有良好的表现。\n",
    "\n",
    "#### 目前，主要两种迁移学习方法如下：\n",
    "\n",
    "- **特征提取（feature extraction）**：特征提取是使用之前网络学到的表示来从新样本中提取出有趣的特征。然后将这些特征输入一个新的分类器，从头开始训练。\n",
    "用于图像分类的卷积神经网络包含两部分：首先是一系列池化层和卷积层，最后是一个密集连接分类器。第一部分叫作模型的卷积基（convolutional base）。对于卷积神经网络而言，特征提取就是取出之前训练好的网络的卷积基，在上面运行新数据，然后在输出上面练一个新的分类器。\n",
    "\n",
    "![](feature_extraction.png)\n",
    "\n",
    "**我们将冻结除最后一个全连接层之外的所有网络的权重。最后一个全连接层被替换为具有随机权重的新层，并且仅训练该层。**举个列子，用一个在ImageNet上预先训练的卷积层，删除最后一个完全连接的层(该层的输出是ImageNet等不同任务的1000个类别的得分)，然后将ConvNet的其余部分作为新数据集的固定特征提取器。\n",
    "\n",
    "- **微调模型（fine-tuning）**：与特征提取互为补充。**对于用于特征提取的冻结的模型基，微调是指将其顶部的几层“解冻”，并将这解冻的几层和新增加的部分（本例中是全连接分类器）联合训练**,之所以叫作微调，是因为它只是略微调整了所复用模型中更加抽象的表示，以便让这些表示与手头的问题更加相关。\n",
    "\n",
    "使用预训练网络权重初始化网络，而不是随机初始化，就像在ImageNet 1000数据集上训练的网络一样。举个例子，ImageNet数据集，它有1000个不同的类别，因此这个网络会有一个Softmax单元，它可以输出1000个可能类别之一。你可以去掉这个 Softmax层，创建你自己的Softmax单元，比如你想识别“猫还是狗”，那只有2个类别，可以把最后一层softmax输出从1000替换成2。通常这样你的网络只训练最后一层softmax层，而其他层都会被冻结(freeze)，也就是不参与训练，这样你的网络会获得非常好的性能，即使在一个非常小的数据集下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 微调模型（fine-tuning）\n",
    "使用预训练模型，并随机初始化最后一个全连接层。\n",
    "\n",
    "核心代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to C:\\Users\\Administrator/.torch\\models\\resnet18-5c106cde.pth\n",
      "46827520it [00:43, 1080533.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# torchvision工具中包含了很多预训练模型，这里直接用resnet18模型，注意pretrained=True\n",
    "model_ft = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name:conv1.weight\tsize:torch.Size([64, 3, 7, 7])\n",
      "name:bn1.weight\tsize:torch.Size([64])\n",
      "name:bn1.bias\tsize:torch.Size([64])\n",
      "name:layer1.0.conv1.weight\tsize:torch.Size([64, 64, 3, 3])\n",
      "name:layer1.0.bn1.weight\tsize:torch.Size([64])\n",
      "name:layer1.0.bn1.bias\tsize:torch.Size([64])\n",
      "name:layer1.0.conv2.weight\tsize:torch.Size([64, 64, 3, 3])\n",
      "name:layer1.0.bn2.weight\tsize:torch.Size([64])\n",
      "name:layer1.0.bn2.bias\tsize:torch.Size([64])\n",
      "name:layer1.1.conv1.weight\tsize:torch.Size([64, 64, 3, 3])\n",
      "name:layer1.1.bn1.weight\tsize:torch.Size([64])\n",
      "name:layer1.1.bn1.bias\tsize:torch.Size([64])\n",
      "name:layer1.1.conv2.weight\tsize:torch.Size([64, 64, 3, 3])\n",
      "name:layer1.1.bn2.weight\tsize:torch.Size([64])\n",
      "name:layer1.1.bn2.bias\tsize:torch.Size([64])\n",
      "name:layer2.0.conv1.weight\tsize:torch.Size([128, 64, 3, 3])\n",
      "name:layer2.0.bn1.weight\tsize:torch.Size([128])\n",
      "name:layer2.0.bn1.bias\tsize:torch.Size([128])\n",
      "name:layer2.0.conv2.weight\tsize:torch.Size([128, 128, 3, 3])\n",
      "name:layer2.0.bn2.weight\tsize:torch.Size([128])\n",
      "name:layer2.0.bn2.bias\tsize:torch.Size([128])\n",
      "name:layer2.0.downsample.0.weight\tsize:torch.Size([128, 64, 1, 1])\n",
      "name:layer2.0.downsample.1.weight\tsize:torch.Size([128])\n",
      "name:layer2.0.downsample.1.bias\tsize:torch.Size([128])\n",
      "name:layer2.1.conv1.weight\tsize:torch.Size([128, 128, 3, 3])\n",
      "name:layer2.1.bn1.weight\tsize:torch.Size([128])\n",
      "name:layer2.1.bn1.bias\tsize:torch.Size([128])\n",
      "name:layer2.1.conv2.weight\tsize:torch.Size([128, 128, 3, 3])\n",
      "name:layer2.1.bn2.weight\tsize:torch.Size([128])\n",
      "name:layer2.1.bn2.bias\tsize:torch.Size([128])\n",
      "name:layer3.0.conv1.weight\tsize:torch.Size([256, 128, 3, 3])\n",
      "name:layer3.0.bn1.weight\tsize:torch.Size([256])\n",
      "name:layer3.0.bn1.bias\tsize:torch.Size([256])\n",
      "name:layer3.0.conv2.weight\tsize:torch.Size([256, 256, 3, 3])\n",
      "name:layer3.0.bn2.weight\tsize:torch.Size([256])\n",
      "name:layer3.0.bn2.bias\tsize:torch.Size([256])\n",
      "name:layer3.0.downsample.0.weight\tsize:torch.Size([256, 128, 1, 1])\n",
      "name:layer3.0.downsample.1.weight\tsize:torch.Size([256])\n",
      "name:layer3.0.downsample.1.bias\tsize:torch.Size([256])\n",
      "name:layer3.1.conv1.weight\tsize:torch.Size([256, 256, 3, 3])\n",
      "name:layer3.1.bn1.weight\tsize:torch.Size([256])\n",
      "name:layer3.1.bn1.bias\tsize:torch.Size([256])\n",
      "name:layer3.1.conv2.weight\tsize:torch.Size([256, 256, 3, 3])\n",
      "name:layer3.1.bn2.weight\tsize:torch.Size([256])\n",
      "name:layer3.1.bn2.bias\tsize:torch.Size([256])\n",
      "name:layer4.0.conv1.weight\tsize:torch.Size([512, 256, 3, 3])\n",
      "name:layer4.0.bn1.weight\tsize:torch.Size([512])\n",
      "name:layer4.0.bn1.bias\tsize:torch.Size([512])\n",
      "name:layer4.0.conv2.weight\tsize:torch.Size([512, 512, 3, 3])\n",
      "name:layer4.0.bn2.weight\tsize:torch.Size([512])\n",
      "name:layer4.0.bn2.bias\tsize:torch.Size([512])\n",
      "name:layer4.0.downsample.0.weight\tsize:torch.Size([512, 256, 1, 1])\n",
      "name:layer4.0.downsample.1.weight\tsize:torch.Size([512])\n",
      "name:layer4.0.downsample.1.bias\tsize:torch.Size([512])\n",
      "name:layer4.1.conv1.weight\tsize:torch.Size([512, 512, 3, 3])\n",
      "name:layer4.1.bn1.weight\tsize:torch.Size([512])\n",
      "name:layer4.1.bn1.bias\tsize:torch.Size([512])\n",
      "name:layer4.1.conv2.weight\tsize:torch.Size([512, 512, 3, 3])\n",
      "name:layer4.1.bn2.weight\tsize:torch.Size([512])\n",
      "name:layer4.1.bn2.bias\tsize:torch.Size([512])\n",
      "name:fc.weight\tsize:torch.Size([1000, 512])\n",
      "name:fc.bias\tsize:torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "for name,param in model_ft.named_parameters():\n",
    "    print(\"name:{}\\tsize:{}\".format(name,param.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取得全连接层的输入单元个数\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "# 微调模型，替换全连接层:把全连接层的输出神经元个数改为2\n",
    "model_ft.fc = nn.Linear(num_ftrs, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征提取（feature extraction）\n",
    "在这里，我们需要冻结除最后一层之外的所有网络。我们需要设置冻结参数requires_grad == False,以便不计算反向传播backward()的时候不计算梯度。\n",
    "\n",
    "核心代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接使用torchvison的预训练模型，这里可换成resnet34,resnet50,vgg16等ImageNet预训练模型\n",
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# 冻结除最后一层之外的所有网络\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 取得全连接层的输入单元个数\n",
    "# 把全连接层的输出神经元个数改为2\n",
    "# 默认情况下新层的requires_grad=True\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你能熟练的掌握迁移学习，那么深度学习领域中计算机识别的问题都会变得简单，可以尝试做一些简单的入门级别的kaggle竞赛了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
