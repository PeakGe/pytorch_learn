{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据和模型迁移到GPU上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 如何进行迁移"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**对模型和相应的数据进行.cuda()处理。通过这种方式，我们就可以将内存中的数据复制到GPU的显存中去。从而可以通过GPU来进行运算了。**\n",
    "\n",
    "网上说的非常简单，但是实际使用过程中还是遇到了一些疑惑。下面分数据和模型两方面的迁移来进行说明介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 判定使用GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载了对应的GPU版本的Pytorch之后，要确保GPU是可以进行使用的，**通过torch.cuda.is_available()的返回值来进行判断。返回True则具有能够使用的GPU。 \n",
    "通过torch.cuda.device_count()可以获得能够使用的GPU数量。其他就不多赘述了。** \n",
    "\n",
    "常常通过如下判定来写可以跑在GPU和CPU上的通用模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "    ten1_cuda = ten1.cuda()\n",
    "    MyModel_cuda = MyModel.cuda() \n",
    "else:\n",
    "    torch.set_default_tensor_type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 对应数据的迁移\n",
    "\n",
    "数据方面常用的主要是两种 —— Tensor和Variable。实际上这两种类型是同一个东西，因为Variable实际上只是一个容器，这里先视其不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 将Tensor迁移到显存中去\n",
    "\n",
    "不论是什么类型的Tensor（FloatTensor或者是LongTensor等等），一律直接使用方法.cuda()即可。 \n",
    "例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten1 = torch.FloatTensor(2)\n",
    ">>>>  6.1101e+24\n",
    "      4.5659e-41\n",
    "      [torch.FloatTensor of size 2]\n",
    "\n",
    "ten1_cuda = ten1.cuda()\n",
    ">>>>   6.1101e+24\n",
    "       4.5659e-41\n",
    "       [torch.cuda.FloatTensor of size 2 (GPU 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其数据类型会由torch.FloatTensor变为torch.cuda.FloatTensor （GPU 0）这样代表这个数据现在存储在 \n",
    "GPU 0的显存中了。 \n",
    "如果要将显存中的数据复制到内存中，则对cuda数据类型使用.cpu()方法即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 将Variable迁移到显存中去"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在模型中，我们最常使用的是Variable这个容器来装载使用数据。主要是由于Variable可以进行反向传播来进行自动求导。 \n",
    "同样地，要将Variable迁移到显存中，同样只需要使用.cuda()即可实现。\n",
    "\n",
    "这里有一个小疑问，对Variable直接使用.cuda和对Tensor进行.cuda然后再放置到Variable中结果是否一致呢。答案是肯定的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ten1 = torch.FloatTensor(2)\n",
    ">>>  6.1101e+24\n",
    "     4.5659e-41\n",
    "    [torch.FloatTensor of size 2]\n",
    "\n",
    "ten1_cuda = ten1.cuda()\n",
    ">>>>  6.1101e+24\n",
    "      4.5659e-41\n",
    "    [torch.cuda.FloatTensor of size 2 (GPU 0)]\n",
    "\n",
    "V1_cpu = autograd.Variable(ten1)\n",
    ">>>> Variable containing:\n",
    "     6.1101e+24\n",
    "     4.5659e-41\n",
    "    [torch.FloatTensor of size 2]\n",
    "\n",
    "V2 = autograd.Variable(ten1_cuda)\n",
    ">>>> Variable containing:\n",
    "     6.1101e+24\n",
    "     4.5659e-41\n",
    "    [torch.cuda.FloatTensor of size 2 (GPU 0)]\n",
    "\n",
    "V1 = V1_cpu.cuda()\n",
    ">>>> Variable containing:\n",
    "     6.1101e+24\n",
    "     4.5659e-41\n",
    "    [torch.cuda.FloatTensor of size 2 (GPU 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终我们能发现他们都能够达到相同的目的，但是他们完全一样了吗？我们使用V1 is V2发现，结果是否定的。\n",
    "\n",
    "对于V1，我们是直接对Variable进行操作的,这样子V1的.grad_fn中会记录下创建的方式。因此这二者并不是完全相同的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 数据迁移小结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".cuda()操作默认使用GPU 0也就是第一张显卡来进行操作。当我们想要存储在其他显卡中时可以使用.cuda(<显卡号数>)来将数据存储在指定的显卡中。还有很多种方式，具体参考官方文档。\n",
    "\n",
    "对于不同存储位置的变量，我们是不可以对他们直接进行计算的。存储在不同位置中的数据是不可以直接进行交互计算的。 \n",
    "换句话说也就是上面例子中的torch.FloatTensor是不可以直接与torch.cuda.FloatTensor进行基本运算的。位于不同GPU显存上的数据也是不能直接进行计算的。\n",
    "\n",
    "对于Variable，其实就仅仅是一种能够记录操作信息并且能够自动求导的容器，实际上的关键信息并不在Variable本身，而更应该侧重于Variable中存储的data。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 模型迁移\n",
    "\n",
    "模型的迁移这里指的是torch.nn下面的一些网络模型以及自己创建的模型迁移到GPU上去。\n",
    "上面讲了使用.cuda()即可将数据从内存中移植到显存中去。 \n",
    "对于模型来说，也是同样的方式，我们使用.cuda来将网络放到显存上去。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 torch.nn下的基本模型迁移"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = torch.FloatTensor(2)\n",
    "data2 = data1.cuda\n",
    "\n",
    "# 创建一个输入维度为2，输出维度为2的单层神经网络\n",
    "linear = torch.nn.Linear(2, 2)\n",
    ">>>> Linear(in_features=2, out_features=2)\n",
    "\n",
    "linear_cuda = linear.cuda()\n",
    ">>>> Linear(in_features=2, out_features=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们很惊奇地发现对于模型来说，不像数据那样使用了.cuda()之后会改变其的数据类型。模型看起来没有任何的变化。 \n",
    "但是他真的没有改变吗。 \n",
    "我们将data1投入linear_cuda中去可以发现，系统会报错，而将.cuda之后的data2投入linear_cuda才能正常工作。并且输出的也是具有cuda的数据类型。\n",
    "\n",
    "那是怎么一回事呢？ \n",
    "这是因为这些所谓的模型，其实也就是对输入参数做了一些基本的矩阵运算。所以我们对模型.cuda()实际上也相当于将模型使用到的参数存储到了显存上去。\n",
    "\n",
    "对于上面的例子，我们可以通过观察参数来发现区别所在。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear.weight\n",
    ">>>> Parameter containing:\n",
    "    -0.6847  0.2149\n",
    "    -0.5473  0.6863\n",
    "    [torch.FloatTensor of size 2x2]\n",
    "\n",
    "linear_cuda.weight\n",
    ">>>> Parameter containing:\n",
    "    -0.6847  0.2149\n",
    "    -0.5473  0.6863\n",
    "    [torch.cuda.FloatTensor of size 2x2 (GPU 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 自己模型的迁移"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于自己创建的模型类，由于继承了torch.nn.Module，则可同样使用.cuda()来将模型中用到的所有参数都存储到显存中去。\n",
    "\n",
    "这里笔者曾经有一个疑问：当我们对模型存储到显存中去之后，那么这个模型中的方法后面所创建出来的Tensor是不是都会默认变成cuda的数据类型。答案是否定的。具体操作留给读者自己去实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pytorch使用指定GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 直接在终端指定:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=1 python my_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 python代码中设定："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
