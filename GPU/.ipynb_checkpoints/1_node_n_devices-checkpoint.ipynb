{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset,DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 模型的数据并行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数和数据加载\n",
    "input_size = 5\n",
    "output_size = 2\n",
    "\n",
    "batch_size = 30\n",
    "data_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.伪数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset):\n",
    "    def __init__(self,size,length):\n",
    "        self.length=length\n",
    "        self.data=torch.randn(length,size)\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDataset=RandomDataset(input_size, data_size)\n",
    "rand_loader = DataLoader(dataset=myDataset,\n",
    "                         batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.简单模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(Model,self).__init__()\n",
    "        self.fc=nn.Linear(input_size,output_size)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        output=self.fc(input)\n",
    "        print(\" In Model:input size:\",input.size(),\n",
    "                 \"output size\", output.size())\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.创建模型和 DataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_size, output_size)\n",
    "model = nn.DataParallel(model)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修改： \n",
    "- 基于对实验结果的观察， 多卡训练的基本过程是（可能不严谨）：\n",
    "    - 首先将模型加载到一个指定设备上作为 controller, 然后将模型**浅复制**到多个设备中，将**大batch 数据**也**等分**到不同设备中， 每个设备分配到不同的数据，然后将所有设备计算得到**梯度合并**用以更新 controller 模型的参数。\n",
    "\n",
    "- 我们需要修改两个地方：\n",
    "\n",
    "    - **model = nn.DataParallel(model)**会将模型**浅复制** 到**所有**可用的显卡中（如果是我实验室的服务器，就是复制到 8 张卡中）,我们希望只占用显卡 1 和 3, 所以需要传入参数 device_ids=[1,3]\n",
    "    - **model = model.cuda()**会将模型加载到 0 号显卡并作为 controller. 但是我们并不打算使用 0 号显卡。所以需要修改为:**model = model.cuda(device_ids[0])**, 即我们将模型加载 1 号显卡并作为 controller。\n",
    "    \n",
    "综上，上面这段代码修改为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_size, output_size)\n",
    "model = nn.DataParallel(model,device_ids=[1,3])#将model浅拷贝到device_ids指定的显卡中\n",
    "model = model.cuda(device_ids[0])#将model加载到cuda(...)指定的显卡，并作为controller，cuda():默认0号显卡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.运行模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in rand_loader:\n",
    "    #将batsize份数据加载到cuda(...)指定的显卡中\n",
    "    input_var=data.cuda()\n",
    "    print(\"Outside: input size\", input_var.size(),\n",
    "          \"output_size\", output.size())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修改：\n",
    "\n",
    "这里也有一个大坑。input_var = Variable(data.cuda()) 会将整个 batch 数据加载到 0 号卡，显然需要修改。但是应该加载到哪呢 ？ 上面我们已经把模型加载到 1 号卡并且作为 controller ,  如果再把整个batch 的数据加载到 1 号卡，照理讲显存应该不够啊。一张卡不能同时放下模型和大batch的数据，这是在文首说明的进行多卡训练的动机啊。\n",
    "\n",
    "于是我将数据加载到 3 号卡， 但是在前向传播时报错了：\n",
    "\n",
    "**all tensors should be in device[0]**\n",
    "\n",
    "也就是说我们需要先将数据加载到 1 号卡（起码在代码层是这样的，物理层就不清楚了）。\n",
    "\n",
    "综上，将代码修改为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in rand_loader:\n",
    "    if torch.cuda.is_available():\n",
    "        input_var = data.cuda(device[0])\n",
    "    else:\n",
    "        input_var = data\n",
    "    output = model(input_var)\n",
    "    print(\"Outside: input size\", input_var.size(),\n",
    "          \"output_size\", output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "  In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "#-----------------------------------------------------------------------------------------\n",
    "  In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "  In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "\n",
    "#-----------------------------------------------------------------------------------------\n",
    "  In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "  In Model: input size torch.Size([15, 5]) output size torch.Size([15, 2])\n",
    "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
    "#-----------------------------------------------------------------------------------------\n",
    "  In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n",
    "  In Model: input size torch.Size([5, 5]) output size torch.Size([5, 2])\n",
    "Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n",
    "#-----------------------------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在 逻辑层:\n",
    "    - 100 个数据被分成  4 批, 每批包含的样本数为 [30, 30, 30, 10]， 不妨记为 batch0,  batch1,  batch2,  batch3\n",
    "- 在 物理层:\n",
    "    - batch0 的数据在前向传播时， 被 等分 到显卡 1 和 3上， 不妨记为 batch00,  batch01,   包含的样本数均为 30 ÷ 2 = 15\n",
    "    - batch1 的数据在前向传播时， 被 等分 到显卡 1 和 3上， 不妨记为 batch10,  batch11,   包含的样本数均为 30 ÷ 2 = 15\n",
    "    - batch2 的数据在前向传播时， 被 等分 到显卡 1 和 3上， 不妨记为 batch20,  batch21,   包含的样本数均为 30 ÷ 2 = 15\n",
    "    - batch3 的数据在前向传播时， 被 等分 到显卡 1 和 3上， 不妨记为 batch30,  batch31,   包含的样本数均为 10 ÷ 2 = 5.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 多GPU运行保存加载恢复checkpoint的几个关键"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分：认识多GPU的DataParalle model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第1层：认识model本身"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 类型1：\n",
    "如果是cpu model或单GPU model\n",
    "- **2种形式(sequential model和sequential&OrderedDict model)**\n",
    "    - 一种是sequential model如下，引用具体层的方式是model[index]，因为sequential会自动对layers编号，可类似于list切片方式调用\n",
    "    ```\n",
    "    Sequential(\n",
    "          (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
    "          (1): ReLU()\n",
    "          (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
    "          (3): ReLU())\n",
    "\n",
    "    ```\n",
    "    - 另一种是sequential&OrderedDict model，引用引用方式是model.conv1，因为sequential针对ordereddict进行了优化可以直接通过.来调用层\n",
    "    ```\n",
    "    Sequential(\n",
    "          (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
    "          (relu1): ReLU()\n",
    "          (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
    "          (relu2): ReLU())\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TheModelClass(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "class TheModelClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TheModelClass,self).__init__()\n",
    "        self.conv1=nn.Conv2d(3,6,5)\n",
    "        self.pool=nn.MaxPool2d(2,2)\n",
    "        self.conv2=nn.Conv2d(6,16,5)\n",
    "        self.fc1=nn.Linear(16*5*5,120)\n",
    "        self.fc2=nn.Linear(120,84)\n",
    "        self.fc3=nn.Linear(84,10)\n",
    "    def farward(self,x):\n",
    "        x=self.pool(F.relu(self.conv1(x)))\n",
    "        x=self.pool(F.relu(self.conv2(x)))\n",
    "        x=x.view(-1,16*5*5)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "# Initialize model\n",
    "model=TheModelClass()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 类型2：\n",
    "\n",
    "**如果是data paralle model：可以看到是在原model基础上wrap了一个module外壳如下**\n",
    "\n",
    "引用方式类似OrderedDict model的嵌套：\n",
    "- model.module就是引用内部的真实模型\n",
    "- model.module.name就是引用具体层(前提是sequential内包含了OrderedDict)\n",
    "- model.module[index]也是引用具体层(前提是sequential内不包含OrderedDict)\n",
    "\n",
    "```\n",
    "DataParallel(\n",
    "      (module): ResNet(\n",
    "        (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        …))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第2层：认识checkpoint文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过torch.save(name, dir)保存的就叫checkpoint文件，可以存一个dict或存一个state_dict (OrderedDict)。一般的dict文件用来保存模型运行的状态信息和参数，state_dict用来保存参数是dict的一部分。如下是一个典型的checkpoint数据结构\n",
    "```\n",
    "{meta: dict\n",
    "state_dict: OrderedDict\n",
    "optimizer: dict}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第3层：认识state_dict字典\n",
    "- state_dict必然是一个OrderedDict数据类型，保存的内容就是所有深度学习需要优化的参数。\n",
    "\n",
    "- **如果是Data paralle model在模型的state_dict中保存的内容会额外增加以module作为开头**\n",
    "\n",
    "```\n",
    "odict_keys(['module.features.0.weight', \n",
    "\t\t      'module.features.0.bias', \n",
    "\t\t      'module.features.3.weight', \n",
    "\t\t      'module.features.3.bias', \n",
    "\t\t      'module.classifier.0.weight', \n",
    "\t             'module.classifier.0.bias', \n",
    "\t\t      'module.classifier.2.weight', \n",
    "\t\t      'module.classifier.2.bias', \n",
    "\t\t      'module.classifier.4.weight', \n",
    "\t\t      'module.classifier.4.bias'])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二部分：处理模型的保存和加载的流程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.save:\n",
    "\n",
    "- 通过torch.save(name, dir)完成，先组合checkpoint, 然后保存\n",
    "\n",
    "- 方案1：直接save state_dict(OrderedDict)，且只save state_dict，后边只是用于inference。这样就会有2种形式state_dict存在，一种不带module前缀，一种带module前缀。\n",
    "注：state_dict需要先.cpu()\n",
    "\n",
    "- 方案2：直接save state_dict(OrderedDict)，且只save state_dict，后边只是后边用于inference。**但state_dict格式都统一成不带module的形式。**\n",
    "注：torch.save(model.state_dict, filepath) 此时保存的是OrderedDict，但可能有两种形式\n",
    "**注：torch.save(model.module.state_dict, filepath)此时保存的也是state_dict，但data paralle不会再带有module前缀**\n",
    "\n",
    "- 方案3：间接save整个training status(dict)，**不只save state_dict，还save epoch/iter/optimizer state_dict等状态参数，后边用于回复训练，这个dict需要自己组合**\n",
    "\n",
    "- **方案3是最常用的方案，因为他适用范围更广，不但可以training也可以inference**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpus：[0,1,,2,3]\n",
    "#将model浅拷贝到gpus\n",
    "#将model加载到0号显卡，并作为controller\n",
    "model = nn.DataParallel(model, device_ids=gpus).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([{'params':\n",
    "                                          filter(lambda p: p.requires_grad,#只更新requires_grad=True的参数\n",
    "                                                 model.parameters()),\n",
    "                              'lr': config.TRAIN.LR}],#0.01\n",
    "                            lr=config.TRAIN.LR,#0.01\n",
    "                            momentum=config.TRAIN.MOMENTUM,#0.9\n",
    "                            weight_decay=config.TRAIN.WD,#0.0005\n",
    "                            nesterov=config.TRAIN.NESTEROV,#False\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将模型和optimizer的state_dict等信息(指定key:value)保存到checkpoint.pth.tar文件\n",
    "torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'best_mIoU': best_mIoU,\n",
    "            'state_dict': model.module.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "        }, os.path.join(final_output_dir, 'checkpoint.pth.tar'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. load checkpoint："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通过torch.load(checkpoint, map_location)完成\n",
    "\n",
    "    - 通过map_location控制加载位置：\n",
    "        - 可以加载到cpu： map_location = lambda storage, loc: storage\n",
    "        - 可以加载到GPU: map_location = lambda storage, loc: storage.cuda(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存了模型和optimizer的state_dict等信息的文件:checkpoint.pth.tar\n",
    "model_state_file = os.path.join(final_output_dir,'checkpoint.pth.tar')\n",
    "checkpoint = torch.load(model_state_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 获得state_dict："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1)获得checkpoint后需要对checkpoint判断后处理获得state_dict：\n",
    "\n",
    "- 如果checkpoint是OrderedDict，那么可以直接得到state_dict\n",
    "- 如果checkpoint是dict，那么可以从字典中获得state_dict\n",
    "- 如果state_dict包含module前缀，那么需要先去除module前缀，下面是一种处理前缀方式\n",
    "\n",
    "```\n",
    "if list(state_dict.keys())[0].startswith('module.'):\n",
    "    state_dict = {k[7:]: v for k, v in checkpoint['state_dict'].items()}\n",
    "\n",
    "```\n",
    "核心原因：saving DataParallel wrapped model can cause problems when the model_state_dict is loaded into an unwrapped model. 保存了wrapped model然后加载到unwrapped model所以必然出错。(**我们上面的代码model.module.state_dict不会再带有module前缀,通过model.module.load_state_dict()获取)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. load state dict："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 通过load_state_dict(model， state_dict)完成\n",
    "\n",
    "- (1)模型加载state_dict前需要对model判断处理：\n",
    "    - 如果是不带module的model，则加载不带module的state_dict：load_state_dict(model, state_dict)\n",
    "    - 如果是带module的model，则取内层model(相当于去掉module)然后加载不带module的state_dict，如下：load_state_dict(model.module， state_dict)\n",
    "\n",
    "关键：**带module wrap的model，可以直接加载带module前缀的state_dict，此时model和state_dict都不需要做去module化处理，model.load_state_dict(state_dict_w/_module)即可**；当然也可以model和state_dict同时做去module化处理，model.module.load_state_dict(state_dict_w/o_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载模型保存的state_dict,指定key\n",
    "model.module.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
