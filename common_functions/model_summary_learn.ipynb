{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取模型的总参数量，总计算量，每1类操作的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model每1层的参数信息元组的列表\n",
    "summary = []\n",
    "#每1层对应的具名元组\n",
    "ModuleDetails = namedtuple(\"Layer\", [\"name\", \"input_size\", \"output_size\", \"num_parameters\", \"multiply_adds\"])\n",
    "#每1层对应的hook列表\n",
    "hooks = []\n",
    "#k-v:网络层类名-出现的次数\n",
    "layer_instances = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算每1类操作的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#网络层module的类名\n",
    "class_name = str(module.__class__.__name__)\n",
    "\n",
    "instance_index = 1\n",
    "if class_name not in layer_instances:\n",
    "    layer_instances[class_name] = instance_index\n",
    "else:\n",
    "    instance_index = layer_instances[class_name] + 1\n",
    "    layer_instances[class_name] = instance_index\n",
    "\n",
    "#网络层名字=网络层类名_出现的次数\n",
    "layer_name = class_name + \"_\" + str(instance_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算某1层的参数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#参数数量\n",
    "params = 0\n",
    "#卷积层/Batch norm层/Linear层\n",
    "if class_name.find(\"Conv\") != -1 or class_name.find(\"BatchNorm\") != -1 or \\\n",
    "   class_name.find(\"Linear\") != -1:\n",
    "    #params：module.parameters()参数的数量\n",
    "    for param_ in module.parameters():\n",
    "        params += param_.view(-1).size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算某1层的计算量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算量\n",
    "flops = \"Not Available\"\n",
    "\n",
    "#Conv层：\n",
    "# input:长度为1的元组，元组中的元素类型为Tensor的size=[bs,num_channels_in,height_in,width_in]\n",
    "# output:Tensor size=[bs,num_channels_out,height_out,width_out]\n",
    "#Conv层:计算量=卷积核参数个数*输出size\n",
    "if class_name.find(\"Conv\") != -1 and hasattr(module, \"weight\"):\n",
    "    #.item():获取标量tensor的元素值\n",
    "    flops = (\n",
    "        torch.prod(\n",
    "            torch.LongTensor(list(module.weight.data.size()))) *\n",
    "        torch.prod(\n",
    "            torch.LongTensor(list(output.size())[2:]))).item()\n",
    "#Linear层\n",
    "#input:长度为1的元组，元组中的元素类型为Tensor的size=[bs,num_in]\n",
    "#output:Tensor size=[bs,num_out]\n",
    "#Linear层:计算量=输出的size*输入的num_in\n",
    "elif isinstance(module, nn.Linear):\n",
    "    flops = (torch.prod(torch.LongTensor(list(output.size()))) \\\n",
    "             * input[0].size(1)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将每1层信息append到summary列表中\n",
    "summary.append(\n",
    "    ModuleDetails(\n",
    "        name=layer_name,\n",
    "        #不记录num_channel\n",
    "        input_size=list(input[0].size()),\n",
    "        output_size=list(output.size()),\n",
    "        num_parameters=params,\n",
    "        multiply_adds=flops)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "**module.register_forward_hook(hook)：hook-钩子函数：**https://www.cnblogs.com/hellcat/p/8512090.html\n",
    "**model.apply(add_hooks):apply函数：将add_hooks函数递归地应用到网络模型model的每1层module中**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hooks(module):\n",
    "\n",
    "    def hook(module, input, output):\n",
    "        #网络层module的类名\n",
    "        class_name = str(module.__class__.__name__)\n",
    "\n",
    "        instance_index = 1\n",
    "        if class_name not in layer_instances:\n",
    "            layer_instances[class_name] = instance_index\n",
    "        else:\n",
    "            instance_index = layer_instances[class_name] + 1\n",
    "            layer_instances[class_name] = instance_index\n",
    "\n",
    "        #网络层名字=网络层类名_出现的次数\n",
    "        layer_name = class_name + \"_\" + str(instance_index)\n",
    "\n",
    "        #参数数量\n",
    "        params = 0\n",
    "        #卷积层/Batch norm层/Linear层\n",
    "        if class_name.find(\"Conv\") != -1 or class_name.find(\"BatchNorm\") != -1 or \\\n",
    "           class_name.find(\"Linear\") != -1:\n",
    "            #params：module.parameters()参数的数量\n",
    "            for param_ in module.parameters():\n",
    "                params += param_.view(-1).size(0)\n",
    "\n",
    "        #计算量\n",
    "        flops = \"Not Available\"\n",
    "        #卷积层:计算量=卷积核参数个数*输出size\n",
    "        if class_name.find(\"Conv\") != -1 and hasattr(module, \"weight\"):\n",
    "            #.item():获取标量tensor的元素值\n",
    "            flops = (\n",
    "                torch.prod(\n",
    "                    torch.LongTensor(list(module.weight.data.size()))) *\n",
    "                torch.prod(\n",
    "                    torch.LongTensor(list(output.size())[2:]))).item()\n",
    "        #Conv层：\n",
    "        # input:长度为1的元组，元组中的元素类型为Tensor的size=[bs,num_channels_in,height_in,width_in]\n",
    "        # output:Tensor size=[bs,num_channels_out,height_out,width_out]\n",
    "\n",
    "        #Linear层\n",
    "        #input:长度为1的元组，元组中的元素类型为Tensor的size=[bs,num_in]\n",
    "        #output:Tensor size=[bs,num_out]\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            flops = (torch.prod(torch.LongTensor(list(output.size()))) \\\n",
    "                     * input[0].size(1)).item()\n",
    "\n",
    "        if isinstance(input[0], list):\n",
    "            input = input[0]\n",
    "\n",
    "        if isinstance(output, list):\n",
    "            output = output[0]\n",
    "\n",
    "        #将每1层信息append到summary列表中\n",
    "        summary.append(\n",
    "            ModuleDetails(\n",
    "                name=layer_name,\n",
    "                #不记录num_channel\n",
    "                input_size=list(input[0].size()),\n",
    "                output_size=list(output.size()),\n",
    "                num_parameters=params,\n",
    "                multiply_adds=flops)\n",
    "        )\n",
    "\n",
    "    #module是model的某一层：卷积层/Batch norm层/Linear层\n",
    "    if not isinstance(module, nn.ModuleList) \\\n",
    "       and not isinstance(module, nn.Sequential) \\\n",
    "       and module != model:\n",
    "        #module每次前向传播执行结束后会执行钩子函数(hook)\n",
    "        #钩子函数不应修改输入和输出，并且在使用后应及时删除，以避免每次都运行钩子增加运行负载\n",
    "        hooks.append(module.register_forward_hook(hook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2a52f2bc5356>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#model.eval()模式:不做batch norm和dropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#apply(fn)：将fn函数递归地应用到网络模型的每1层中，主要用在参数的初始化。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#model.eval()模式:不做batch norm和dropout\n",
    "model.eval()\n",
    "#apply(fn)：将fn函数递归地应用到网络模型的每1层中，主要用在参数的初始化。\n",
    "model.apply(add_hooks)\n",
    "\n",
    "space_len = item_length\n",
    "\n",
    "model(*input_tensors)\n",
    "for hook in hooks:\n",
    "    hook.remove()\n",
    "    \n",
    "details=''\n",
    "#总参数量\n",
    "params_sum = 0\n",
    "#总计算量\n",
    "flops_sum = 0\n",
    "for layer in summary:\n",
    "    params_sum += layer.num_parameters\n",
    "    if layer.multiply_adds != \"Not Available\":\n",
    "        flops_sum += layer.multiply_adds\n",
    "#os.linesep：终止符(换行)\n",
    "details += os.linesep \\\n",
    "    + \"Total Parameters: {:,}\".format(params_sum) \\\n",
    "    + os.linesep + '-' * space_len * 5 + os.linesep\n",
    "details += \"Total Multiply Adds (For Convolution and Linear Layers only): {:,} GFLOPs\".format(flops_sum/(1024**3)) \\\n",
    "    + os.linesep + '-' * space_len * 5 + os.linesep\n",
    "\n",
    "details += \"Number of Layers\" + os.linesep\n",
    "#记录每1类层出现的次数\n",
    "for layer in layer_instances:\n",
    "    details += \"{} : {} layers   \".format(layer, layer_instances[layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
