{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "链接：https://www.jianshu.com/p/4905bf8e06e5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVING AND LOADING MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当提到保存和加载模型时，有三个核心功能需要熟悉：\n",
    "\n",
    "1. torch.save：将序列化的对象保存到disk。这个函数使用Python的pickle实用程序进行序列化。使用这个函数可以保存各种对象的模型、张量和字典。\n",
    "2. torch.load：使用pickle unpickle工具将pickle的对象文件反序列化为内存。\n",
    "3. torch.nn.Module.load_state_dict:使用反序列化状态字典加载model’s参数字典。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一：WHAT IS A STATE_DICT\n",
    "\n",
    "在PyTorch中，torch.nn.Module的可学习参数(即权重和偏差)，模块模型包含在model's参数中(通过model.parameters()访问)。state_dict是个简单的Python dictionary对象，它将每个层映射到它的参数张量。\n",
    "\n",
    "**注意，只有具有可学习参数的层(卷积层、线性层等)才有model's state_dict中的条目。优化器对象(connector .optim)也有一个state_dict，其中包含关于优化器状态以及所使用的超参数的信息.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TheModelClass(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Define model\n",
    "class TheModelClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TheModelClass,self).__init__()\n",
    "        self.conv1=nn.Conv2d(3,6,5)\n",
    "        self.pool=nn.MaxPool2d(2,2)\n",
    "        self.conv2=nn.Conv2d(6,16,5)\n",
    "        self.fc1=nn.Linear(16*5*5,120)\n",
    "        self.fc2=nn.Linear(120,84)\n",
    "        self.fc3=nn.Linear(84,10)\n",
    "    def farward(self,x):\n",
    "        x=self.pool(F.relu(self.conv1(x)))\n",
    "        x=self.pool(F.relu(self.conv2(x)))\n",
    "        x=x.view(-1,16*5*5)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "# Initialize model\n",
    "model=TheModelClass()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\tTrue\n",
      "conv1.bias\tTrue\n",
      "conv2.weight\tTrue\n",
      "conv2.bias\tTrue\n",
      "fc1.weight\tTrue\n",
      "fc1.bias\tTrue\n",
      "fc2.weight\tTrue\n",
      "fc2.bias\tTrue\n",
      "fc3.weight\tTrue\n",
      "fc3.bias\tTrue\n",
      "\n",
      "Model's state_dict:\n",
      "conv1.weight \t torch.Size([6, 3, 5, 5])\n",
      "conv1.bias \t torch.Size([6])\n",
      "conv2.weight \t torch.Size([16, 6, 5, 5])\n",
      "conv2.bias \t torch.Size([16])\n",
      "fc1.weight \t torch.Size([120, 400])\n",
      "fc1.bias \t torch.Size([120])\n",
      "fc2.weight \t torch.Size([84, 120])\n",
      "fc2.bias \t torch.Size([84])\n",
      "fc3.weight \t torch.Size([10, 84])\n",
      "fc3.bias \t torch.Size([10])\n",
      "\n",
      "optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.0001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [1988292414920, 1988292413624, 1988292414776, 1988292412904, 1988292370792, 1988292371944, 1988292370576, 1988292370936, 1988292373888, 1988292372232]}]\n"
     ]
    }
   ],
   "source": [
    "for name,param in model.named_parameters():\n",
    "    print(name+'\\t'+str(param.requires_grad))\n",
    "    #print(type(param))\n",
    "# Initialize optimizer\n",
    "optimizer=torch.optim.SGD(model.parameters(),lr=1e-4,momentum=0.9)\n",
    "\n",
    "print(\"\\nModel's state_dict:\")\n",
    "# Print model's state_dict\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor,\"\\t\",model.state_dict()[param_tensor].size())\n",
    "print(\"\\noptimizer's state_dict:\")\n",
    "# Print optimizer's state_dict\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name,\"\\t\",optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二：SAVING & LOADING MODEL FOR INFERENCE\n",
    "\n",
    "### Save/Load state_dict (Recommended)--只保存参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save:\n",
    "```\n",
    "  torch.save(model.state_dict(), PATH)\n",
    "```\n",
    "\n",
    "\n",
    "在保存模型进行推理时，只需要保存训练过的模型的学习参数即可。一个常见的PyTorch约定是使用.pt或.pth文件扩展名保存模型。\n",
    "\n",
    "\n",
    "- Load:\n",
    "```\n",
    " model = TheModelClass(*args, **kwargs)\n",
    " model.load_state_dict(torch.load(PATH))\n",
    " model.eval()\n",
    "```\n",
    "\n",
    "\n",
    "记住，您必须调用model.eval()，以便在运行推断之前将dropout和batch规范化层设置为评估模式。如果不这样做，将会产生不一致的推断结果。\n",
    "\n",
    "**Note:\n",
    " 注意，load_state_dict()函数接受一个dictionary对象，而不是保存对象的路径。这意味着您必须在将保存的state_dict传至load_state_dict()函数之前反序列化torch.load()它。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Load Entire Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Save:\n",
    "```\n",
    "  torch.save(model, PATH)\n",
    "```\n",
    "\n",
    "\n",
    "- Load:\n",
    "```\n",
    "#Model class must be defined somewhere\n",
    "  model = torch.load(PATH)\n",
    " model.eval()\n",
    " ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三：Example1："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "        ...\n",
    "        }, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TheModelClass(*args, **kwargs)\n",
    "optimizer = TheOptimizerClass(*args, **kwargs)\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "model.eval()\n",
    "# - or -\n",
    "model.train()</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在保存用于推理或恢复训练的通用检查点时，必须保存模型的state_dict。另外，保存优化器的state_dict也是很重要的，因为它包含缓冲区和参数，这些缓冲区和参数是在模型训练时更新的。要保存多个组件，请将它们组织在字典中，并使用torch.save()序列化字典。一个常见的PyTorch约定是使用.tar文件扩展名保存这些检查点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example2:初始化参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class torch.nn.Parameter()\n",
    "一种Variable，被视为一个模块参数。\n",
    "\n",
    "- Parameters 是 Variable 的子类。当与Module一起使用时，它们具有非常特殊的属性，当它们被分配为模块属性时，它们被自动添加到其参数列表中，并将出现在例如parameters()迭代器中。分配变量没有这样的效果。这是因为人们可能希望在模型中缓存一些临时状态，如RNN的最后一个隐藏状态。如果没有这样的班级Parameter，这些临时人员也会注册。\n",
    "\n",
    "- 另一个区别是，parameters不能是volatile，他们默认要求梯度。\n",
    "\n",
    "- 参数说明:\n",
    "\n",
    "    - data (Tensor) – parameter tensor.\n",
    "\n",
    "    - requires_grad (bool, optional) – 默认True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(self, pretrained='',):\n",
    "    logger.info('=> init weights from normal distribution')\n",
    "    for m in self.modules():\n",
    "        #Conv2d层参数初始化:正态分布weight\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.normal_(m.weight, std=0.001)\n",
    "        #BatchNorm层参数初始化:常量weight=1,bias=0\n",
    "        elif isinstance(m, InPlaceABNSync):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    #加载预训练模型的参数\n",
    "    if os.path.isfile(pretrained):\n",
    "        #先反序列化预训练模型保存的预训练参数字典\n",
    "        pretrained_dict = torch.load(pretrained)\n",
    "        logger.info('=> loading pretrained model {}'.format(pretrained))\n",
    "        #当前模型所有参数的字典\n",
    "        model_dict = self.state_dict()\n",
    "        #在预训练参数字典中保留当前模型存在的参数\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items()\n",
    "                           if k in model_dict.keys()}\n",
    "        for k, _ in pretrained_dict.items():\n",
    "            logger.info(\n",
    "                '=> loading {} pretrained model {}'.format(k, pretrained))\n",
    "        #更新当前模型参数字典\n",
    "        model_dict.update(pretrained_dict)\n",
    "        #加载\n",
    "        self.load_state_dict(model_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
