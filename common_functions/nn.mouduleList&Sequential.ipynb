{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于cnn前馈神经网络如果前馈一次写一个forward函数会有些麻烦，**在此就有两种简化方式，ModuleList和Sequential。**\n",
    "其中\n",
    "\n",
    "- Sequential是一个特殊的module，它包含几个子Module，前向传播时会将输入一层接一层的传递下去。\n",
    "- ModuleList也是一个特殊的module，可以包含几个子module，可以像用list一样使用它，但不能直接把输入传给ModuleList。下面举例说明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.nn.ModuleList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以把任意 nn.Module 的子类 (比如 nn.Conv2d, nn.Linear 之类的) 加到这个 list 里面，方法和 Python 自带的 list 一样，无非是 extend，append 等操作。\n",
    "**但不同于一般的 list，加入到 nn.ModuleList 里面的 module 是会自动注册到整个网络上的，同时 module 的 parameters 也会自动添加到整个网络中。**\n",
    "\n",
    "描述看起来很枯燥，我们来看几个例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 第一个网络，我们先来看看使用 nn.ModuleList 来构建一个小型网络，包括3个全连接层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net1, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10,10) for i in range(2)])\n",
    "    def forward(self, x):\n",
    "        for m in self.linears:\n",
    "            x = m(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net1(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = net1()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linears.0.weight <class 'torch.Tensor'> torch.Size([10, 10])\n",
      "linears.0.bias <class 'torch.Tensor'> torch.Size([10])\n",
      "linears.1.weight <class 'torch.Tensor'> torch.Size([10, 10])\n",
      "linears.1.bias <class 'torch.Tensor'> torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name,param in net.named_parameters():\n",
    "    print(name,type(param.data),param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到，这个网络包含两个全连接层，他们的权重 (weithgs) 和偏置 (bias) 都在这个网络之内。\n",
    "\n",
    "#### 接下来我们看看第二个网络，它使用 Python 自带的 list："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net2, self).__init__()\n",
    "        self.linears = [nn.Linear(10,10) for i in range(2)]\n",
    "    def forward(self, x):\n",
    "        for m in self.linears:\n",
    "            x = m(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net2()\n"
     ]
    }
   ],
   "source": [
    "net = net2()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(list(net.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然，**使用 Python 的 list 添加的全连接层和它们的 parameters 并没有自动注册到我们的网络中。**当然，我们还是可以使用 forward 来计算输出结果。但是如果用 net2 实例化的网络进行训练的时候，**因为这些层的 parameters 不在整个网络之中，所以其网络参数也不会被更新，也就是无法训练。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好，看到这里，我们大致明白了 nn.ModuleList 是干什么的了：它是一个储存不同 module，并自动将每个 module 的 parameters 添加到网络之中的容器。但是，我们需要注意到，nn.ModuleList 并没有定义一个网络，它只是将不同的模块储存在一起，这些模块之间并没有什么先后顺序可言，比如：\n",
    "\n",
    "**第3个网络**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net3, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10,20), nn.Linear(20,30), nn.Linear(5,10)])\n",
    "    def forward(self, x):\n",
    "        x = self.linears[2](x)\n",
    "        x = self.linears[0](x)\n",
    "        x = self.linears[1](x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net3(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=20, bias=True)\n",
      "    (1): Linear(in_features=20, out_features=30, bias=True)\n",
      "    (2): Linear(in_features=5, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = net3()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 30])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(32, 5)\n",
    "print(net(input).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据 net3 的结果，**我们可以看出来这个 ModuleList 里面的顺序并不能决定什么，网络的执行顺序是根据 forward 函数来决定的。如果你非要 ModuleList 和 forward 中的顺序不一样， PyTorch 表示它无所谓，但以后 review 你代码的人可能会意见比较大。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**我们再考虑另外一种情况，既然这个 ModuleList 可以根据序号来调用，那么一个模块是否可以在 forward 函数中被调用多次呢？答案当然是可以的，但是，被调用多次的模块，是使用同一组 parameters 的，也就是它们的参数是共享的，无论之后怎么更新。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net4, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(5, 10), nn.Linear(10, 10)])\n",
    "    def forward(self, x):\n",
    "        x = self.linears[0](x)\n",
    "        x = self.linears[1](x)\n",
    "        x = self.linears[1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net4(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=5, out_features=10, bias=True)\n",
      "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = net4()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linears.0.weight torch.Size([10, 5])\n",
      "linears.0.bias torch.Size([10])\n",
      "linears.1.weight torch.Size([10, 10])\n",
      "linears.1.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.nn.Sequential()对象\n",
    "\n",
    "现在我们来研究一下 nn.Sequential，不同于 nn.ModuleList，它已经实现了内部的 forward 函数，而且里面的模块必须是按照顺序进行排列的，所以我们必须确保前一个模块的输出大小和下一个模块的输入大小是一致的.\n",
    "\n",
    "如下面的例子所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net5, self).__init__()\n",
    "        self.block = nn.Sequential(nn.Conv2d(1,20,5),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(20,64,5),\n",
    "                                    nn.ReLU())\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net5(\n",
      "  (block): Sequential(\n",
      "    (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = net5()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 模型的建立方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 写法1\n",
    "\n",
    "**nn.Sequential()对象.add_module(层名，层class的实例）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1=nn.Sequential()\n",
    "net1.add_module('conv1',nn.Conv2d(1,20,5))\n",
    "net1.add_module('relu1',nn.ReLU())\n",
    "net1.add_module('conv2',nn.Conv2d(20,64,5))\n",
    "net1.add_module('activation',nn.ReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (activation): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model1 和 从类 net5 实例化来的 net 有什么区别吗？\n",
    "\n",
    "是没有的。这两个网络是相同的，因为 nn.Sequential 就是一个 nn.Module 的子类，也就是 nn.Module 所有的方法 (method) 它都有。\n",
    "\n",
    "并且**直接使用 nn.Sequential 不用写 forward 函数，因为它内部已经帮你写好了。**\n",
    "\n",
    "**如果你确定 nn.Sequential 里面的顺序是你想要的，而且不需要再添加一些其他处理的函数，那么完全可以直接用 nn.Sequential。这么做的代价就是失去了部分灵活性，毕竟不能自己去定制 forward 函数里面的内容了。**\n",
    "\n",
    "一般情况下 nn.Sequential 的用法是来组成卷积块 (block)，然后像拼积木一样把不同的 block 拼成整个网络，让代码更简洁，更加结构化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 写法2\n",
    "\n",
    "**nn.Sequential(*多个层class的实例,....)**:没有层名，只有序号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2=nn.Sequential(\n",
    "    nn.Conv2d(3,3,3),\n",
    "    nn.BatchNorm2d(3),\n",
    "    nn.ReLU()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 写法3\n",
    "\n",
    "**nn.Sequential(OrderedDict(\\[*多个(层名，层class的实例)\\]),...)**\n",
    "\n",
    "OrderedDict 来指定每个 module 的名字，而不是采用默认的命名方式 (按序号 0,1,2,3...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "net3=nn.Sequential(OrderedDict([\n",
    "        ('conv',nn.Conv2d(3,3,3)),\n",
    "        ('batchnorm',nn.BatchNorm2d(3)),\n",
    "        ('activation',nn.ReLU())\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 检查以及调用模型\n",
    "#### 查看模型\n",
    "print对象即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net1:Sequential(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (conv2): Conv2d(20, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (activation): ReLU()\n",
      ")\n",
      "net2:<class '__main__.net2'>\n",
      "net3:Sequential(\n",
      "  (conv): Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (batchnorm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('net1:{}'.format(net1))\n",
    "print('net2:{}'.format(net2))\n",
    "print('net3:{}'.format(net3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 根据名字或序号提取子Module对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 3, kernel_size=(3, 3), stride=(1, 1))\n",
      "BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "ReLU()\n"
     ]
    }
   ],
   "source": [
    "print(net1.conv)\n",
    "print(net2[1])\n",
    "print(net3.activation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 调用模型\n",
    "\n",
    "可以直接网络对象(输入数据)，也可以使用上面的Module子对象分别传入(input)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.0000, 0.5194],\n",
      "          [1.2274, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.4375],\n",
      "          [0.0000, 1.2353]],\n",
      "\n",
      "         [[0.0000, 0.0000],\n",
      "          [0.0000, 0.1895]]]], grad_fn=<ThresholdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input=torch.randn(1,3,4,4)\n",
    "output = net3(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.ModuleList 和 nn.Sequential: 到底该用哪个\n",
    "前边我们已经简单介绍了这两个类，现在我们来讨论一下在两个不同的场景中，选择哪一个比较合适。\n",
    "\n",
    "**场景一，有的时候网络中有很多相似或者重复的层，我们一般会考虑用 for 循环来创建它们，而不是一行一行地写，**\n",
    "\n",
    "比如：``layers = [nn.Linear(10, 10) for i in range(5)]``\n",
    "\n",
    "这个时候，很自然而然地，我们会想到使用 ModuleList，像这样："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net6(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net6, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(3)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.linears:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net6(\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = net6()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个是比较一般的方法，但如果不想这么麻烦，我们也可以用 Sequential 来实现，如 net7 所示！\n",
    "\n",
    "**注意 * 这个操作符，它可以把一个 list 拆开成一个个独立的元素。但是，请注意这个 list 里面的模块必须是按照想要的顺序来进行排列的。**\n",
    "\n",
    "在 场景一 中，我个人觉得使用 net7 这种方法比较方便和整洁。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net7(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net7, self).__init__()\n",
    "        self.linears_list = [nn.Linear(10, 10) for i in range(3)]\n",
    "        self.linears = nn.Sequential(*self.linears_list)\n",
    "    \n",
    "    #forward中直接调用nn.Sequential对象\n",
    "    def forward(self, x):\n",
    "        self.x = self.linears(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net7(\n",
      "  (linears): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = net7()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们考虑 场景二，当我们需要之前层的信息的时候，**比如 ResNets 中的 shortcut 结构，或者是像 FCN 中用到的 skip architecture 之类的，当前层的结果需要和之前层中的结果进行融合，一般使用 ModuleList 比较方便，**\n",
    "\n",
    "一个非常简单的例子如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net8(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net8, self).__init__()\n",
    "        self.linears = nn.ModuleList([nn.Linear(10, 20), nn.Linear(20, 30), nn.Linear(30, 50)])\n",
    "        self.trace = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.linears:\n",
    "            x = layer(x)\n",
    "            self.trace.append(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 20])\n",
      "torch.Size([32, 30])\n",
      "torch.Size([32, 50])\n"
     ]
    }
   ],
   "source": [
    "net = net8()\n",
    "input  = torch.randn(32, 10) # input batch size: 32\n",
    "output = net(input)\n",
    "for each in net.trace:\n",
    "    print(each.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们使用了一个 trace 的列表来储存网络每层的输出结果，这样如果以后的层要用的话，就可以很方便地调用了。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
