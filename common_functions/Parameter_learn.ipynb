{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自动求导机制\n",
    "### Backward过程中排除子图\n",
    "\n",
    "pytorch的BP过程是由一个函数决定的，``loss.backward()``， 可以看到backward()函数里并没有传要求谁的梯度。那么我们可以大胆猜测，在BP的过程中，pytorch是将所有影响loss的Variable都求了一次梯度。**但是有时候，我们并不想求所有Variable的梯度。那就要考虑如何在Backward过程中排除子图（ie.排除没必要的梯度计算）。**\n",
    "\n",
    "如何BP过程中排除子图？ \n",
    "\n",
    "**Variable的两个参数（requires_grad和volatile）**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### requires_grad\n",
    "\n",
    "如果你想部分冻结你的网络（ie.不做梯度计算），那么通过设置requires_grad标签是非常容易实现的。 \n",
    "下面给出了利用requires_grad使用pretrained网络的一个例子，只fine tune了最后一层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.randn(5, 5))\n",
    "y = Variable(torch.randn(5, 5))\n",
    "z = Variable(torch.randn(5, 5), requires_grad=True)\n",
    "a = x + y  # x, y的 requires_grad的标记都为false， 所以输出的变量requires_grad也为false\n",
    "b=x+z\n",
    "print(a.requires_grad)\n",
    "print(b.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个标志特别有用，当您想要冻结部分模型时，或者您事先知道不会使用某些参数的梯度。例如，如果要对预先训练的CNN进行优化，只要切换冻结模型中的requires_grad标志就足够了，直到计算到最后一层才会保存中间缓冲区，其中的仿射变换将使用需要梯度的权重并且网络的输出也将需要它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "#冻结参数，不参与backward\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#替换全连接层\n",
    "model.fc = nn.Linear(512, 100)\n",
    "\n",
    "#只有fc层参数\n",
    "optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### volatile\n",
    "纯粹的inference模式下推荐使用volatile，当你确定你甚至不会调用.backward()时。它比任何其他自动求导的设置更有效——它将使用绝对最小的内存来评估模型。**volatile也决定了require_grad is False。**\n",
    "\n",
    "**volatile=True相当于requires_grad=False。**\n",
    "\n",
    "但是在纯推断模式的时候，只要操作有一个的输入volatile=True，它的输出也将是volatile=True。这就比使用requires_grad=False方便多了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class torch.nn.Parameter()\n",
    "一种Variable，被视为一个模块参数。\n",
    "\n",
    "- Parameters 是 Variable 的子类。当与Module一起使用时，它们具有非常特殊的属性，**当它们被分配为模块属性时，它们被自动添加到其参数列表中，并将出现在例如parameters()迭代器中。**分配变量没有这样的效果。\n",
    "\n",
    "- 解释：**在Module中的层在定义时，相关Variable的requires_grad参数默认是True。 在计算图中，如果有一个输入的requires_grad是True，那么输出的requires_grad也是True。只有在所有输入的requires_grad都为False时，输出的requires_grad才为False。**\n",
    "\n",
    "- 另一个区别是，parameters不能是volatile，他们默认要求梯度。\n",
    "\n",
    "- 参数说明:\n",
    "\n",
    "    - data (Tensor) – parameter tensor.\n",
    "\n",
    "    - requires_grad (bool, optional) – 默认True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.分配Tensor时，默认Tensor的requires_grad属性为False，可以设置为True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensor和Variable已经合并了\n",
    "x=torch.randn((2,3,4))\n",
    "y=torch.randn((4,3,2),requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x))\n",
    "print(x.size())\n",
    "print(x.requires_grad)\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.在Module中的层在定义时，相关Tensor的requires_grad属性默认是True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class TheModelClass(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TheModelClass,self).__init__()\n",
    "        self.conv1=nn.Conv2d(3,6,5)\n",
    "        self.pool=nn.MaxPool2d(2,2)\n",
    "        self.conv2=nn.Conv2d(6,16,5)\n",
    "        self.fc1=nn.Linear(16*5*5,120)\n",
    "        self.fc2=nn.Linear(120,84)\n",
    "        self.fc3=nn.Linear(84,10)\n",
    "    def farward(self,x):\n",
    "        x=self.pool(F.relu(self.conv1(x)))\n",
    "        x=self.pool(F.relu(self.conv2(x)))\n",
    "        x=x.view(-1,16*5*5)\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        x=self.fc3(x)\n",
    "        return x\n",
    "# Initialize model\n",
    "model=TheModelClass()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1model.parameters()\n",
    "- 迭代model.parameters()将会返回每一次迭代元素的param，**可以用来改变requires_grad的属性**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  param in model.parameters():\n",
    "    print(param.requires_grad)\n",
    "    #修改requires_grad的属性为False\n",
    "    param.requires_grad=False\n",
    "    \n",
    "for  param in model.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2model.named_parameters()\n",
    "- 迭代model.named_parameters()将会返回每一次迭代元素的name和param的元组，**可以用来改变requires_grad的属性**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for  name,param in model.named_parameters():\n",
    "    print(name+'\\t'+str(param.requires_grad))\n",
    "    #修改requires_grad的属性为False\n",
    "    param.requires_grad=False\n",
    "    \n",
    "for  name,param in model.named_parameters():\n",
    "    print(name+'\\t'+str(param.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 model.state_dict().items()\n",
    "- 每次迭代model.state_dict().items()会返回所有的name和param，**但是这里的所有的param都是requires_grad=False,没有办法改变requires_grad的属性，所以改变requires_grad的属性只能通过上面的两种方式。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.state_dict().items():\n",
    "    print(name+'\\t'+str(param.requires_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 改变了requires_grad之后要修改optimizer的属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(\n",
    "            parameters=filter(lambda p: p.requires_grad, model.parameters()),   #只更新requires_grad=True的参数\n",
    "            lr=cfg.TRAIN.LR,\n",
    "            momentum=cfg.TRAIN.MOMENTUM,\n",
    "            weight_decay=cfg.TRAIN.WD,\n",
    "            nesterov=cfg.TRAIN.NESTEROV\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
