{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        #conv1: 16 * 5 * 5保持宽高的卷积(stride=1, padding=2), relu，2 * 2的maxpool\n",
    "        self.conv1=nn.Sequential(\n",
    "            #卷积函数\n",
    "            nn.Conv2d(in_channels=3,out_channels=16,kernel_size=5,stride=1,padding=2),#===>[16,28,28]\n",
    "            nn.ReLU(),                                                                  #===>[16,28,28]\n",
    "            #池化函数\n",
    "            nn.MaxPool2d(kernel_size=2)                                                 #===>[16,14,14]\n",
    "        )\n",
    "        #conv2:32*5*5保持宽高的卷积(stride=1, padding=2), relu，2 * 2的maxpool\n",
    "        self.conv2=nn.Sequential(\n",
    "            nn.Conv2d(16,32,5,1,2),                                                     #===>[32,14,14]\n",
    "            nn.ReLU(),                                                                  #===>[32,14,14]\n",
    "            nn.MaxPool2d(kernel_size=2)                                                 #===>[32,7,7]\n",
    "        )\n",
    "        #全连接层：[32*7*7,10]\n",
    "        self.out=nn.Linear(32*7*7,10)\n",
    "\n",
    "    #重写forward()\n",
    "    def forward(self, x):\n",
    "        x=self.conv1(x)\n",
    "\n",
    "        x=self.conv2(x)\n",
    "        print(x.shape)\n",
    "        #全连接之前，展平,保留batch的维度[batch,32,7,7]==>[batch,32*7*7]\n",
    "        x=x.view(x.size(0),-1)#x.size(0)指batchsize的值,而-1指在不告诉函数有多少列的情况下，根据原tensor数据和batchsize自动分配列数\n",
    "        output=self.out(x)\n",
    "        return output,x\n",
    "\n",
    "#8.搭建CNN\n",
    "model=CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.使用初始化函数初始化参数\n",
    "\n",
    "遍历模型的每1层，根据网络层的不同定义不同的初始化方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init_1(model):\n",
    "    for m in model.modules:\n",
    "        #Linear层初始化\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_normal_(m.weight)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        # 也可以判断是否为conv2d，使用相应的初始化方式 \n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "         # 是否为批归一化层\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.从文件中加载预训练模型保存的参数\n",
    "\n",
    "1. 从文件路径中反序列化预训练参数字典:torch.load()\n",
    "2. 获取当前模型的参数字典:model.state_dict()\n",
    "3. 在预训练参数字典中保留当前模型存在的参数\n",
    "4. 更新当前模型参数字典\n",
    "5. 当前模型加载更新后的参数字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init_2(model,pretrain_path):\n",
    "    \n",
    "    #加载预训练模型的参数\n",
    "    if os.path.isfile(pretrain_path):\n",
    "        #先反序列化预训练模型保存的预训练参数字典\n",
    "        pretrained_dict = torch.load(pretrain_path)\n",
    "        #当前模型所有参数的字典\n",
    "        model_dict = model.state_dict()\n",
    "        #在预训练参数字典中保留当前模型存在的参数\n",
    "        pretrained_dict = {k: v for k, v in pretrained_dict.items()\n",
    "                           if k in model_dict.keys()}\n",
    "        #更新当前模型参数字典\n",
    "        model_dict.update(pretrained_dict)\n",
    "        #加载\n",
    "        model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        #conv1: 16 * 5 * 5保持宽高的卷积(stride=1, padding=2), relu，2 * 2的maxpool\n",
    "        self.conv1=nn.Sequential(\n",
    "            #卷积函数\n",
    "            nn.Conv2d(in_channels=3,out_channels=16,kernel_size=5,stride=1,padding=2),#===>[16,28,28]\n",
    "            nn.ReLU(),                                                                  #===>[16,28,28]\n",
    "            #池化函数\n",
    "            nn.MaxPool2d(kernel_size=2)                                                 #===>[16,14,14]\n",
    "        )\n",
    "        #conv2:32*5*5保持宽高的卷积(stride=1, padding=2), relu，2 * 2的maxpool\n",
    "        self.conv2=nn.Sequential(\n",
    "            nn.Conv2d(16,32,5,1,2),                                                     #===>[32,14,14]\n",
    "            nn.ReLU(),                                                                  #===>[32,14,14]\n",
    "            nn.MaxPool2d(kernel_size=2)                                                 #===>[32,7,7]\n",
    "        )\n",
    "        #全连接层：[32*7*7,10]\n",
    "        self.out=nn.Linear(32*7*7,10)\n",
    "\n",
    "    #重写forward()\n",
    "    def forward(self, x):\n",
    "        x=self.conv1(x)\n",
    "\n",
    "        x=self.conv2(x)\n",
    "        print(x.shape)\n",
    "        #全连接之前，展平,保留batch的维度[batch,32,7,7]==>[batch,32*7*7]\n",
    "        x=x.view(x.size(0),-1)#x.size(0)指batchsize的值,而-1指在不告诉函数有多少列的情况下，根据原tensor数据和batchsize自动分配列数\n",
    "        output=self.out(x)\n",
    "        return output,x\n",
    "    \n",
    "        def init_weights(self, pretrained='',):\n",
    "        logger.info('=> init weights from normal distribution')\n",
    "        #参数初始化\n",
    "        for m in self.modules():\n",
    "            #Conv2d层参数初始化:正态分布weight\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, std=0.001)\n",
    "            #BatchNorm层参数初始化:常量weight=1,bias=0\n",
    "            elif isinstance(m, InPlaceABNSync):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        #加载预训练模型的参数\n",
    "        if os.path.isfile(pretrained):\n",
    "            #先反序列化预训练模型保存的预训练参数字典\n",
    "            pretrained_dict = torch.load(pretrained)\n",
    "            logger.info('=> loading pretrained model {}'.format(pretrained))\n",
    "            #当前模型所有参数的字典\n",
    "            model_dict = self.state_dict()\n",
    "            #在预训练参数字典中保留当前模型存在的参数\n",
    "            pretrained_dict = {k: v for k, v in pretrained_dict.items()\n",
    "                               if k in model_dict.keys()}\n",
    "            for k, _ in pretrained_dict.items():\n",
    "                logger.info(\n",
    "                    '=> loading {} pretrained model {}'.format(k, pretrained))\n",
    "            #更新当前模型参数字典\n",
    "            model_dict.update(pretrained_dict)\n",
    "            #加载\n",
    "            self.load_state_dict(model_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
